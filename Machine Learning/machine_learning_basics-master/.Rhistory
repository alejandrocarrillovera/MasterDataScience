install.packages("gradDescent")
knitr::opts_chunk$set(echo = TRUE)
Sigmoid <- function(x) {
1 / (1 + exp(-x))
}
# feed with data
x <- seq(-5, 5, 0.01)
# and plot
plot(x, Sigmoid(x), col='blue', ylim = c(-.2, 1))
abline(h = 0, v = 0, col = "gray60")
install.packages("here")
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ISLR)
library(MASS)
library(dplyr)
library (here)
install.packages("here")
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ISLR)
library(MASS)
library(dplyr)
library (here)
X4_1_data <- read_csv(here("data/4_1_data.csv"))
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ISLR)
library(MASS)
library(dplyr)
library (here)
library(tidyverse)
X4_1_data <- read_csv(here("data/4_1_data.csv"))
# Gráfico de dispersión
##### Vemos las notas de las dos pruebas, y en color rojo los alumnos admitidos, y en negro los no admitidos.
plot(datos$score1, datos$score2, col = as.factor(datos$label), xlab = "score1", ylab = "score2")
library(readr)
library(dplyr)
library(ggplot2)
library(fastDummies)
loan <- read_csv("basedatos.csv")
DistanceFromPlane = function(z, w, b) {
sum(z * w) + b
}
ClassifyLinear = function(x, w, b) {
distances = apply(x, 1, DistanceFromPlane, w, b)
return(ifelse(distances < 0, -1, +1))
}
EuclideanNorm <- function(x) {
return(sqrt(sum(x * x)))
}
PerceptronFunction <- function(x, y, learning.rate = 1) {
w = vector(length = ncol(x)) # initialize w
b = 0 # Initialize b
iterations = 0 # count iterations
R = max(apply(x, 1, EuclideanNorm))
convergence = FALSE # to enter the while loop
while (!convergence) {
convergence = TRUE # hopes luck
yc <- ClassifyLinear(x, w, b)
for (i in 1:nrow(x)) {
if (y[i] != yc[i]) {
convergence <- FALSE
w <- w + learning.rate * y[i] * x[i,]
b <- b + learning.rate * y[i] * R^2
iterations <- iterations + 1
}
}
}
s = EuclideanNorm(w)
return(list(w = w/s, b = b/s, steps = iterations))
}
# very easy
# x2 = x1 + 1/2
x1 <- runif(50,-1,1)
x2 <- runif(50,-1,1)
x <- cbind(x1,x2)
y <- ifelse(x2 > 0.5 + x1, +1, -1)
PlotData <- function(x, y) {
plot(x, pch = ifelse(y > 0, "+", "-"), xlim = c(-1,1), ylim = c(-1,1), cex = 2)
abline(0.5,1)
points(c(0,0), c(0,0), pch = 19)
lines(c(0,-0.25), c(0,0.25), lty = 2)
arrows(-0.3, 0.2, -0.4, 0.3)
text(-0.45, 0.35, "w", cex = 2)
text(-0.0, 0.15, "b", cex = 2)
}
PlotData(x, y)
# very easy
# x2 = x1 + 1/2
x1 <- runif(50,-1,1)
x2 <- runif(50,-1,1)
x <- cbind(x1,x2)
y <- ifelse(x2 > 0.5 + x1, +1, -1)
PlotData <- function(x, y) {
plot(x, pch = ifelse(y > 0, "+", "-"), xlim = c(-1,1), ylim = c(-1,1), cex = 2)
abline(0.5,1)
points(c(0,0), c(0,0), pch = 19)
lines(c(0,-0.25), c(0,0.25), lty = 2)
arrows(-0.3, 0.2, -0.4, 0.3)
text(-0.45, 0.35, "w", cex = 2)
text(-0.0, 0.15, "b", cex = 2)
}
PlotData(x, y)
the_perceptron <- PerceptronFunction(x,y)
predicted_y <- ClassifyLinear(x, the_perceptron$w, the_perceptron$b)
# error
print(sum(abs(y - predicted_y)))
